{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEAM CW-4 ADVANCED CLASSIFICATION NOTEBOOK\n",
    "This notebook contains the entire workflow of team CW-4 advanced classification predict. The predict involves a project on Twitter sentiment analysis of tweets to get people's perception on climate change. We shall be helping marketing departments to develop marketing strategies based on people's views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "<a href=#one>1. Introduction</a>\n",
    "\n",
    "<a href=#two>2. Importing Packages</a>\n",
    "\n",
    "<a href=#three>3. Loading Data</a>\n",
    "\n",
    "<a href=#four>4. Exploratory Data Analysis</a>\n",
    "\n",
    "<a href=#five>5. Feature Engineering</a>\n",
    "\n",
    "<a href=#six>6. Model Building</a>\n",
    "\n",
    "<a href=#seven>7. Model Evaluation</a>\n",
    "\n",
    "<a href=#one>8. Model Selection</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 1. Introduction\n",
    " <a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "In this notebook, we are going to go through the entire data science workflow to build models, analyze models and select the best model to solve our problem.\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "We are challenged to determine people's perception on climate change, whether they believe that climate change is real and if it is a threat. We shall create a machine learning model that uses natural language processing to determine a person's view on climate change based on their tweet data. We aim to come up with a viable model that is able to accurately classify people into groups of those who believe and those who do not. With this we will be able to offer insights to marketing departments on how well or badly their product will be recieved on the market  based on its effects on the climate. This will help marketing teams to come up with strategies on how to run their campaigns in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The dataset\n",
    "we are provided with a dataset containing tweets collected from 27/04/2015 to 21/02/2018. The dataset contains three features;\n",
    "\n",
    "* sentiment - the class in which a tweet belongs ranging\n",
    " from -1 to 2\n",
    "\n",
    "* message - The body of the tweet provided\n",
    "\n",
    "* tweetid - a unique identifier for each tweet\n",
    "\n",
    "The dataset is split into training data and test data with training data containing 80% of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 2. Importing Packages\n",
    " <a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    " ---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, necessary packages to be used throughout the notebook are imported, and briefly discussed. |\n",
    "| The imported libraries are used in the following stages of the data science process : data cleaning, exploratory data analysis and data modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# import libraries for use in model evaluation\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# import libraries for use in loading data, EDA and data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "#import contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#import advertools as adv\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "import urllib\n",
    "from sklearn.utils import resample\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#import libraries for use in model development\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import libraries for use in model evaluation\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 3. Loading Data\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, the datasets to be used in the modelling process are loaded into DataFrames using the pandas library. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the csv file containing the training data using pandas\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#Load the csv file containing the test data using pandas\n",
    "test_df = pd.read_csv(\"test_with_no_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 4. Exploratory Data Analysis\n",
    " \n",
    " <a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, an in-depth analysis of all the variables in the DataFrame is performed. |\n",
    "| This phase of the project cycle is very important; it offers insight into the data, and any underlying patterns within it, |\n",
    "| as well as any errors, duplicates or outliers present. |\n",
    "| It is essential in understanding the data objectively and guides the data pre-processing and modelling processes. |\n",
    "| The investigations conducted include: the dimensionality of the data, the descriptive statistics, data completeness, | \n",
    "| data distribution, existence of outliers and duplicates, as well as tweet entity extraction, analysis and visualisation.|\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a section of the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset shape\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset has 15,819 rows and 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize data\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the columns are numeric type columns: i.e `sentiment` and `tweetid`; with `sentiment` being the encoded categorical target variable. The other column; `message` is of object type. The output also shows that there are no null values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data statistics - numeric type columns\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics of the numeric variables do not offer much insight. This is because the `tweetid` column is a unique column while the `sentiment` column is an encoded categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at data statistics - object type columns\n",
    "train_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics of the `message` column suggests that there are duplicate tweets in the data. This may be a case of retweets or copied tweets. The most common tweet in the dataset is a retweet(due to the `RT` tag) and it appears 307 times throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of unique values in each column\n",
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tweetid` column contains all-unique values while the `message` column has some duplicate values. The target variable; `sentiment`, contains 4 different class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the duplicate tweets in the message column\n",
    "train_df.loc[train_df['message'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,908 duplicate tweets in the `message` column. These tweets might be duplicates but are all associated with different unique tweet IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['message'].duplicated(keep=False) & train_df['message'].str.contains('RT')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all 1,908 duplicate tweets, majority (i.e 1,899) are retweets as is evidenced by the `RT` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['message'].duplicated(keep=False) & ~train_df['message'].str.contains('RT')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all 1,908 duplicate tweets, 308 of them appear to be copied tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate feature symmetry\n",
    "train_df.skew(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sentiment` column has a moderate negative skew, and this is as a result of the labels chosen for encoding the variable i.e: -1, 0, 1 and 2;\n",
    "while `tweetid` has a fairly symmetrical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate existence of outliers in the data using kurtosis\n",
    "train_df.kurt(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both numeric columns show no evidence of the existence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of tweets in the test and train datasets\n",
    "\n",
    "length_train = train_df['message'].str.len().plot.hist(color = 'grey', figsize = (6, 4))\n",
    "length_test = test_df['message'].str.len().plot.hist(color = 'orange', figsize = (6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pro label is the most frequent category in this dataset; with 8,530 tweets labeled as 1 for supporting belief in man-made climate change, while the least; 1,296 tweets are labeled as -1, for tweets that do not believe in man-made climate change. \n",
    "There is also evidence of class imbalance within our dataset that will need to be remedied to ensure predictability of the model using methods such as resampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution plots of the target variable\n",
    "f = sns.countplot(x='sentiment', data=train_df)\n",
    "f.set_xticklabels(['Anti', 'Neutral', 'Pro', 'News'])\n",
    "f.bar_label(f.containers[0])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Copy Creation and Data Segmentation by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the train data to use for extracting twitter-related information\n",
    "df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subsets of the data as per the label.\n",
    "news_data = df[df['sentiment'] == 2]\n",
    "pro_data = pd.DataFrame(df[df['sentiment'] == 1])\n",
    "neutral_data = df[df['sentiment'] == 0]\n",
    "anti_data = df[df['sentiment'] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Hashtag Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all hashtags from the dataframe using advertools\n",
    "hashtag_summary = adv.extract_hashtags(df['message'])\n",
    "hashtag_summary['overview']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a significant amount of hashtags. Hashtags are popular on Twitter and they are used to index and group tweets around a particular topic. It would be beneficial to explore the popular hashtags in this data and further determine the popular hashtags per tweet category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract hashtags\n",
    "def hashtag_extractor(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function extracts all the hashtags from a collection of tweets using advertools.\n",
    "        Input: a tweet column from a dataframe\n",
    "        Output: a sequence of strings(hashtags) separated by space\n",
    "    \"\"\" \n",
    "    hashtag_summary = adv.extract_hashtags(data)\n",
    "    hashtags = hashtag_summary['hashtags_flat'] #Create a list of all the available hashtags.\n",
    "    tags = (\" \").join(hashtags) #Create a sequence of strings from the hashtags list\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a wordcloud using the extracted entities\n",
    "def wordcloud_visualizer(extracted_entity, color):\n",
    "    \n",
    "     \n",
    "    \"\"\"\n",
    "    This function creates a wordclod visual from a collection of extracted entities.\n",
    "    It uses the WordCloud function from wordcloud to create a wordcloud of the extracted entities with a white background.\n",
    "        Input: a sequence of stings of the extracted entity, preferred color scheme for the wordcloud\n",
    "        Output: a wordcloud visual\n",
    "    \"\"\"  \n",
    "    wordcloud = WordCloud(collocations = False, colormap = color, background_color = 'white').generate(extracted_entity)\n",
    "    \n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the hashtag extractor function to extract hashtags from the different subsets of the data\n",
    "all_tags = hashtag_extractor(df['message'])\n",
    "news_tags = hashtag_extractor(news_data['message'])\n",
    "pro_tags = hashtag_extractor(pro_data['message'])\n",
    "neutral_tags = hashtag_extractor(neutral_data['message'])\n",
    "anti_tags = hashtag_extractor(anti_data['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a wordcloud for all the hashtags available in the full data\n",
    "full_data_cloud = wordcloud_visualizer(all_tags, 'brg')\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(full_data_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Popular Climate Change Hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As would be expected, words such as: `climate`, `climatechange`, `environment`, `actonclimate` and `globalwarming` constitute the most popular hashtags in this data. As previously established, hashtags are used to index and group tweets around a particular topic and the aforementioned hashtags would be the most appropriate tags for the climate change topic.\n",
    "\n",
    "\n",
    "* `BeforetheFlood` is among the most popular climate change hashtags. It is in reference to a film produced by Leonardo DiCaprio that was released in  21 October 2016. The film highlights the dangers of climate change and the possible solutions.\n",
    "\n",
    "\n",
    "* `Imvotingbecause` and `Ivotedbecause` hashtags are prominent in the data as well. Climate change has become a political issue over the decades and is very central to American politics. These hashtags are used throughout the dataset to show support in the now-politicized climate change issue by voters.\n",
    "\n",
    "\n",
    "* `COP22` is a popular hashtag and aligns with the timeframe of the collected data. It stands for the 22nd Session of the Conference of the Parties, the 2016 United Nations Climate Change Conference. It was an international meeting of political leaders and activists to discuss environmental issues, and was held in Marrakech, Morocco, on 7–18 November 2016. Naturally, this conference sparked a lot of debate and conversation on the topic of climate change. \n",
    "\n",
    "\n",
    "* `TheParisAgreement` hashtag aligns with the timeframe of the collected data as well. It is a legally binding international treaty on climate change that was adopted by 196 Parties at COP 21 in Paris, on 12 December 2015. The agreement covers climate change mitigation, adaptation, and finance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated wordclouds for the different labels\n",
    "f, axarr = plt.subplots(2,2, figsize=(35,25))\n",
    "axarr[0,0].imshow(wordcloud_visualizer(news_tags, 'summer'), interpolation=\"bilinear\")\n",
    "axarr[0,1].imshow(wordcloud_visualizer(pro_tags, 'Blues'), interpolation=\"bilinear\")\n",
    "axarr[1,0].imshow(wordcloud_visualizer(neutral_tags, 'Wistia'), interpolation=\"bilinear\")\n",
    "axarr[1,1].imshow(wordcloud_visualizer(anti_tags, 'gist_gray'), interpolation=\"bilinear\")\n",
    "\n",
    "# Remove the ticks on the x and y axarres\n",
    "for ax in f.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.axis('off')\n",
    "\n",
    "axarr[0,0].set_title('News label hashtags\\n', fontsize=50)\n",
    "axarr[0,1].set_title('Pro climate change hashtags\\n', fontsize=50)\n",
    "axarr[1,0].set_title('Neutral label hashtags\\n', fontsize=50)\n",
    "axarr[1,1].set_title('Anti climate change hashtags\\n', fontsize=50)\n",
    "plt.suptitle(\"Climate Change Hashtags by Label\", fontsize = 100)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Trump` is a popular hashtag across the labels in the climate change tweets. Trump's administration saw alot of controversial moves and statements around climate change. Most of his efforts were geared towards dismissing climate change and slowing down efforts to mitigate it; including withdrawal from the 2015 Paris Climate Change agreement where 196 nations pledged to reduce greenhouse gas emissions and assist poor nations struggling with the consequences of global warming. Tweets including this hashtags are most likely centered around peoples' opinions, criticism and/ or support of Trump's views on climate change.\n",
    "\n",
    "\n",
    "* Anti climate change hashtags are laden with former president Trump's slogans and declarations. For example, `DraintheSwamp` and `maga`, which was his slogan during the 2016 campaigns and stands for Make America Great Again. Majority of the hashtags in the anti climate change subset of the data seem to be skeptical and to question the reality of the climate issue e.g `fakenews`, `myth`, `hoax`, `climatescam` and `greenscam`. The inclusion of `tcot` meaning top conservatives on Twitter, suggest that anti climate change tweets are favored by Republican-leaning users.\n",
    "\n",
    "\n",
    "* Pro, News and Neutral tweets seem to share more or less the same hashtags, with Pro tweets having inclusions of vote-related hashtags eg `Imvotingbecause`, News label tweets include broadcast entities eg `CNN` AND `WorldNews`. The apparance of `P2` (Progressives 2.0) hashtags on these labels suggests that these type of tweets are mostly favored by Democrat-leaning users and are used to show progressive political standpoints on Twitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Username Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all mentions from the dataframe using advertools\n",
    "mention_summary = adv.extract_mentions(df['message'])\n",
    "mention_summary['overview']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a significant amount of mentions. These are most-likely politicians and celebrities who have made public their opinions on climate change and are sparking a lot of conversation on the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract mentionss\n",
    "def mentions_extractor(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function extracts all the mentions from a collection of tweets using advertools.\n",
    "        Input: a tweet column from a dataframe\n",
    "        Output: a sequence of strings(mentions) separated by space\n",
    "    \"\"\" \n",
    "    mentions_summary = adv.extract_mentions(data)\n",
    "    mentions = mentions_summary['mentions_flat'] #Create a list of all the available mentions.\n",
    "    usernames = (\" \").join(mentions) #Create a sequence of strings from the mentions list\n",
    "    \n",
    "    return usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mentions extractor function to extract mentionss from the different subsets of the data\n",
    "all_mentions = mentions_extractor(df['message'])\n",
    "news_mentions = mentions_extractor(news_data['message'])\n",
    "pro_mentions = mentions_extractor(pro_data['message'])\n",
    "neutral_mentions = mentions_extractor(neutral_data['message'])\n",
    "anti_mentions = mentions_extractor(anti_data['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a wordcloud for all the mentions available in the full data\n",
    "full_data_cloud = wordcloud_visualizer(all_mentions, 'brg')\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(full_data_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Popular Usernames Mentioned')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Stephen Schlegel` is among the most frequently mentioned usernames in this data. His tweet received alot of excitement after making a quip at former first lady, Melania Trump for her husband's beliefs on climate change. It received a lot of retweets.\n",
    "\n",
    "\n",
    "* The most frequently mentioned users are either politicians or celebrities who have made remarks on climate change that have been met by criticism, support or both by the general public. Celebrities include: `Leo Dicaprio`, `Seth Macfarlane` and `D D Lovato`; politicians include: `Donald Trump`, `Bernie Sanders` and `Kamala Harris`; and Journalists such as `Kurt Eichenwald`. \n",
    "\n",
    "\n",
    "* The other most mentioned entities are broadcast and news channels eg `CNN`, newspaper publications eg `NYTimes` and magazines such as `Mother Jones`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated wordclouds for the different labels\n",
    "f, axarr = plt.subplots(2,2, figsize=(35,25))\n",
    "axarr[0,0].imshow(wordcloud_visualizer(news_mentions, 'summer'), interpolation=\"bilinear\")\n",
    "axarr[0,1].imshow(wordcloud_visualizer(pro_mentions, 'Blues'), interpolation=\"bilinear\")\n",
    "axarr[1,0].imshow(wordcloud_visualizer(neutral_mentions, 'Wistia'), interpolation=\"bilinear\")\n",
    "axarr[1,1].imshow(wordcloud_visualizer(anti_mentions, 'gist_gray'), interpolation=\"bilinear\")\n",
    "\n",
    "# Remove the ticks on the x and y axarres\n",
    "for ax in f.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.axis('off')\n",
    "\n",
    "axarr[0,0].set_title('News label mentions\\n', fontsize=50)\n",
    "axarr[0,1].set_title('Pro climate change mentions\\n', fontsize=50)\n",
    "axarr[1,0].set_title('Neutral label mentions\\n', fontsize=50)\n",
    "axarr[1,1].set_title('Anti climate change mentions\\n', fontsize=50)\n",
    "plt.suptitle(\"Climate Change mentions by Label\", fontsize = 100)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Donald Trump` is the most mentioned person throughout the labels. This could be because of his strong opinions on climate change that are met by equally strong opposition or support by Twitter users.\n",
    "\n",
    "\n",
    "* The news label is characterized by mentions targeting news/information outlets; most prominent being `The Hill`, `NY Times`, `Reuters`, `Washington Post` and `Independent`, all which have exemplary climate news coverage and  Twitter users are likely tagging them or following developing climate change news from them.\n",
    "\n",
    "\n",
    "* The Pro label is characterized by mentions of people who are actively pro climate change while the Anti label is charcterized by mentions of people who are anti climate change e.g `Steve Goddard`, `Dinesh D'souza`, climate change denialists, who have often made remarks that climate change is a hoax. Users are likely trying to interact with those of whom they share the same views on climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script of code extracts all the words in the message column of the dataframe\n",
    "# Create a regular-expressions tokenizing instance\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "# Create a new column of tweet tokens\n",
    "df['text_token']=df['message'].apply(regexp.tokenize)\n",
    "# Make a list of english stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# Remove stopwords\n",
    "df['text_token'] = df['text_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "# Convert to a sequence of strings and keep words longer than one character\n",
    "df['text_token'] = df['text_token'].apply(lambda x: ' '.join([item for item in x if len(item) > 1]))\n",
    "# Create a list of all words\n",
    "all_words = ' '.join([word for word in df['text_token']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(collocations = False, colormap = 'brg', background_color = 'white').generate(all_words)\n",
    "# Display the generated Word Cloud_r\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `https` occurs frequently, implying that many links are being shared around the topic of climate change, most probably by the news label. \n",
    "\n",
    "\n",
    "* The tag `RT` which implies retweet appears frequently meaning that there are alot of shared opinions within the data, which is expected when you have groups of people sharing the same sentiments.\n",
    "\n",
    "\n",
    "* Other common words include climate-specific vocabulary for example: `climate`, `change`, `warming` and `global`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 5. Feature Engineering\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Text Cleaning\n",
    "\n",
    "Removing noise (i.e. unneccesary information) is a key part of getting the data into a usable format.  For this dataset, we will be carrying out the following cleaning techniques:\n",
    "\n",
    "* removing the web urls\n",
    "\n",
    "* removing duplicates\n",
    "\n",
    "* removing usernames\n",
    "\n",
    "* converting all text into lowercase\n",
    "\n",
    "* removing punctuation marks\n",
    "\n",
    "* removing stopwords from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Remove web urls\n",
    "\n",
    "At this point, it is important we clean our test and remove the noise in other to amke it usable. The first thing we will do is to remove the URLs links and replace them with the string `web url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us check for rows with URL links\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "train_df.loc[train_df['message'].str.contains(pattern_url, regex=True )] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the url links with the the text 'web url'\n",
    "subs_url = r'url-web'\n",
    "train_df['message'] = train_df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "train_df.loc[train_df['message'].str.contains(subs_url, regex=True )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['message'].str.contains('https')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Drop duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop_duplicates(subset = 'message', keep = 'first',inplace = True) # drop all duplicate tweets an keep only one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Remove extra space from each Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].str.replace('\\s\\s+', '', regex=True)#extra whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 Remove the retweet tags from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].str.replace('RT', '') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Remove numbers from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].str.replace('\\d+', '', regex=True)#numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 Convert the text into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].str.lower() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 Remove punctuation marks from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_remover(message):\n",
    "    return ''.join([l for l in message if l not in string.punctuation])\n",
    "\n",
    "train_df['message'] = train_df['message'].apply(punc_remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.7 Expand contractions in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message']=train_df['message'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "train_df['message'] = train_df['message'].apply(lambda x: ' '.join([item for item in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview clean dataset\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Test Data\n",
    "\n",
    "Let us carry out the same cleaning we did in the training dataset, but unlike in the training data, we will not be dropping duplicate rows in our test dataset. this is because for the kaggle competition, our dataset must `10546` row entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "test_df.loc[test_df['message'].str.contains(pattern_url, regex=True )]\n",
    "\n",
    "subs_url = r'url-web'\n",
    "test_df['message'] = test_df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "test_df.loc[test_df['message'].str.contains(subs_url, regex=True )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['message'] = test_df['message'].str.replace('RT', '') #remove the retweet tag drom tweets\n",
    "test_df['message'] = test_df['message'].str.lower() # convert the text into lower case\n",
    "test_df['message'] = test_df['message'].str.replace('\\s\\s+', '', regex=True)#extra whitespace\n",
    "test_df['message'] = test_df['message'].str.replace('\\d+', '', regex=True)#numbers\n",
    "test_df['message'] = test_df['message'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "test_df['message'] = test_df['message'].apply(lambda x: ' '.join([item for item in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation marks from the dataset\n",
    "def punc_remover(message):\n",
    "    return ''.join([l for l in message if l not in string.punctuation])\n",
    "\n",
    "test_df['message'] = test_df['message'].apply(punc_remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dealing with Class Imbalance\n",
    "Class imbalance occurs when the number of observations across different class labels are unevenly distributed. To understand if and why we should correct any imbalance in our datset, let's quickly take a look at our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate minority and majority classes\n",
    "news = train_df[train_df['sentiment']==2]\n",
    "pro = train_df[train_df['sentiment']==1]\n",
    "neutral = train_df[train_df['sentiment']==0]\n",
    "anti = train_df[train_df['sentiment']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible labels\n",
    "labels = train_df['sentiment'].unique()\n",
    "heights = [len(news),len(pro),len(neutral),len(anti)]\n",
    "plt.bar(labels,heights,color='orange')\n",
    "plt.xticks(labels,['news','pro','neutral','anti'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a clear imbalance in our label. This is a problem as it can affect the accuracy of our final model. There are three possible approaches we can use to correct this. \n",
    "\n",
    "1. Upsampling the minority class(es)\n",
    "2. Downsampling the majority class(es)\n",
    "3. Upsample minority class + downsample majority class(es)\n",
    "\n",
    "For this dataset, we are going to use Approach #3 which happens to be the best of the three approaches. This technique involves:\n",
    "\n",
    "1. Establishing a **class size** (i.e. the number of observations we want in each class). For this approach to work, the **class size** has to be a value between the size of the majority class and the size of the minority class. A good heuristic to use here, is to **set the class size to be half the size of the majority class**.\n",
    "\n",
    "2. Downsampling the majority class to be as small as the **class size**.\n",
    "\n",
    "3. Upsampling the minority class to be as big as the **class size**.\n",
    "\n",
    "for more effective ness, we are going to create a function for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let create the class size\n",
    "class_size=len(pro)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_classes=[]\n",
    "\n",
    "for label in list(train_df['sentiment'].unique()):\n",
    "    label_data = train_df[train_df['sentiment'] == label]\n",
    "    \n",
    "    if label < class_size:\n",
    "        label_resampled = resample(label_data,\n",
    "                                   replace=True,\n",
    "                                   n_samples=int(class_size),\n",
    "                                   random_state=27) \n",
    "    else:      \n",
    "        label_resampled = resample(label_data,\n",
    "                                   replace=False, # sample without replacement (no need to duplicate observations)\n",
    "                                   n_samples=int(class_size), # match number in minority class\n",
    "                                   random_state=27) # reproducible results\n",
    "    resampled_classes.append(label_resampled)\n",
    "\n",
    "\n",
    "resampled_data = pd.concat(resampled_classes, axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's take a look at our new labels\n",
    "labels = train_df['sentiment'].unique()\n",
    "heights = [len(news),len(pro),len(neutral),len(anti)]\n",
    "resampled_heights= [len(resampled_data[resampled_data['sentiment']==2]),\n",
    "                    len(resampled_data[resampled_data['sentiment']==1]),\n",
    "                    len(resampled_data[resampled_data['sentiment']==0]),\n",
    "                    len(resampled_data[resampled_data['sentiment']==-1])]\n",
    "plt.bar(labels,heights,color='orange')\n",
    "plt.bar(labels,resampled_heights,color='grey')\n",
    "plt.xticks(labels,['news','pro','neutral','anti'])\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have succesfully, balanced our class, let go ahead with our modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Variable Creation\n",
    "\n",
    "At this point, let us extract our features and labels for our modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=resampled_data['message']\n",
    "y=resampled_data['sentiment']\n",
    "X_test=test_df['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_df=0.9, min_df=1, ngram_range=(1, 3))\n",
    "# fit the countvectorizer to the data and \n",
    "#store the results in a variable tokens\n",
    "X = vect.fit_transform(corpus)\n",
    "X_test_df = vect.transform(X_test)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 6. Model Building\n",
    " <a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "Finally! The sweet stuff!\n",
    "\n",
    "In this section, we shall;\n",
    "\n",
    "* Build machine learning models\n",
    "\n",
    "* Fit the machine learning models with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Tree Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize machine learning models\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#fit the model\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#lets predict the label for our test set\n",
    "\n",
    "pred_tree= tree.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's build another models!\n",
    "\n",
    "### 6.2: logistice regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training the logistic regression model on our rebalanced data\n",
    "logreg = LogisticRegression(multi_class='ovr')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "pred_lr = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "  \n",
    "# calculating accuracy score\n",
    "accuracy_score = accuracy_score(pred_lr,y_test)\n",
    "print('accuracy score : ',accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Random classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4: SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "pred_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)  \n",
    "pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc= LinearSVC(C=100)\n",
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate predictions from full model\n",
    "pred_lsvc = lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 7. Model Evaluation\n",
    " <a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "In this section, we shall build the previously developed models on various perfomance metrics such as;\n",
    "\n",
    "* F1 score\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "* Precision \n",
    "\n",
    "* Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tree classification model')\n",
    "print(classification_report(y_test, pred_tree))\n",
    "print('\\n')\n",
    "\n",
    "print('logistice regression model (no selection)')\n",
    "print(classification_report(y_test, pred_lr))\n",
    "print('\\n')\n",
    "\n",
    "print('Random Forest Classification')\n",
    "print(classification_report(y_test, pred_rf))\n",
    "print('\\n')\n",
    "\n",
    "print('SVC model')\n",
    "print(classification_report(y_test, pred_svc))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('KNN Classification model')\n",
    "print(classification_report(y_test, pred_knn))\n",
    "print('\\n')\n",
    "\n",
    "print('Linear SVC Classification model')\n",
    "print(classification_report(y_test, pred_lsvc))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# calculating accuracy score\n",
    "accuracy_score = accuracy_score(pred_lr,y_test)\n",
    "print('accuracy score : ',accuracy_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    " ## 8. Model Selection\n",
    " <a href=#cont>Back to Table of Contents</a>\n",
    " \n",
    "Select the best performing model.\n",
    "\n",
    "we shall select the best performing model based on their accuracy scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tree_classifier=f1_score(y_test,pred_tree, pos_label='positive',average='micro')\n",
    "log_regression=f1_score(y_test,pred_lr, pos_label='positive',average='micro')\n",
    "random_forest=f1_score(y_test,pred_rf, pos_label='positive',average='micro')\n",
    "SVC=f1_score(y_test,pred_svc, pos_label='positive',average='micro')\n",
    "KNN=f1_score(y_test,pred_knn, pos_label='positive',average='micro')\n",
    "LinearSVC=f1_score(y_test,pred_lsvc, pos_label='positive',average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict={'F1_Score':\n",
    "              {\n",
    "           'tree_classifier':tree_classifier,\n",
    "           'log_regression':log_regression,\n",
    "           'random_forest':random_forest,\n",
    "           'SVC':SVC,\n",
    "           'KNN' : KNN,\n",
    "           'LinearSVC' : LinearSVC,\n",
    "              }\n",
    "             }\n",
    "\n",
    "results_df = pd.DataFrame(data=results_dict)\n",
    "# View the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(results_df, y =results_df['F1_Score'],\n",
    "       color = results_df.index, width =700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "from our model evaluation, it looks like our logistic regression model perfromed the best( with a f1 score of 87%).\n",
    "\n",
    "It is possible for this model to perform even better if we carry our further feature engineering and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "pred_test = logreg.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test=tree.predict(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "\n",
    "tweet_id=test_df['tweetid']\n",
    "model_test_df = pd.DataFrame({'sentiment':pred_test,'tweetid':tweet_id})\n",
    "\n",
    "\n",
    "model_test_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "model_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc= LinearSVC()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(LinearSVC(), param_grid)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lsvc_pkl','wb') as files:\n",
    "    pickle.dump(lsvc,files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svc_pkl','wb') as files:\n",
    "    pickle.dump(svc,files)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_pkl','wb') as files:\n",
    "    pickle.dump(rf,files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vect_pkl','wb') as files:\n",
    "    pickle.dump(vect,files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8686c18ed1493b687bf4918dc91855d4eaccc0b5c225ba21d6d4f373ce9d798"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
